# ä¸ªäººçŸ¥è¯†ä»£ç† (Personal Knowledge Agent)

ä¸€ä¸ªåŸºäº RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æŠ€æœ¯çš„æ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼Œæä¾›ç²¾å‡†çš„é—®ç­”æœåŠ¡ã€‚

## ğŸš€ é¡¹ç›®ç‰¹è‰²

- **å¤šæ ¼å¼æ–‡æ¡£æ”¯æŒ**: PDFã€Wordã€Markdownã€æ–‡æœ¬æ–‡ä»¶
- **æ™ºèƒ½åˆ†è¯ä¸æ£€ç´¢**: åŸºäº LangChain çš„é€’å½’æ–‡æœ¬åˆ†å‰²å™¨
- **é«˜æ€§èƒ½å‘é‡å­˜å‚¨**: ä½¿ç”¨ ChromaDB æŒä¹…åŒ–å­˜å‚¨
- **å¤šæ¨¡å‹æ”¯æŒ**: é›†æˆ DeepSeekã€Ollama ç­‰å¤§è¯­è¨€æ¨¡å‹
- **å¤åˆé—®é¢˜å¤„ç†**: è‡ªåŠ¨è¯†åˆ«å¹¶æ‹†åˆ†å¤åˆé—®é¢˜
- **ä¸¥æ ¼è¿‡æ»¤æ¨¡å¼**: ç²¾ç¡®æ–‡æ¡£æŸ¥è¯¢ä¸ç›¸ä¼¼åº¦åŒ¹é…
- **Web ç•Œé¢**: åŸºäº Streamlit çš„ç›´è§‚ç”¨æˆ·ç•Œé¢

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- 8GB+ RAM (æ¨è)
- å¯é€‰: NVIDIA GPU (ç”¨äºæœ¬åœ° Ollama æ¨¡å‹)

## ğŸ›  å®‰è£…é…ç½®

### 1. å…‹éš†é¡¹ç›®

```bash
ç”¨gitå…‹éš†æˆ–ä¸‹è½½é¡¹ç›®åˆ°æœ¬åœ°
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

### 3. å®‰è£…ä¾èµ–

```bash
pip install -r requirements_full.txt
```

### 4. ç¯å¢ƒé…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼Œé…ç½® API å¯†é’¥ï¼š

```env
# å¿…éœ€: Silicon Flow åµŒå…¥æ¨¡å‹ API
SILICON_FLOW_API_KEY=your_silicon_flow_api_key
SILICON_FLOW_API_BASE=https://api.siliconflow.cn/v1

# DeepSeek å¤§è¯­è¨€æ¨¡å‹ (å¯é€‰)
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_API_BASE=https://api.deepseek.com/v1

# Ollama æœ¬åœ°æ¨¡å‹ (å¯é€‰)
OLLAMA_ENABLED=true
OLLAMA_API_BASE=http://localhost:11434
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯åŠ¨åº”ç”¨

```bash
streamlit run app.py
```

åº”ç”¨å°†åœ¨ `http://localhost:8501` å¯åŠ¨ã€‚

### åŸºæœ¬ä½¿ç”¨æµç¨‹

1. **ä¸Šä¼ æ–‡æ¡£**: åœ¨ä¾§è¾¹æ é€‰æ‹©å¹¶ä¸Šä¼ æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
2. **å¤„ç†æ–‡æ¡£**: ç‚¹å‡»"å¤„ç†æ–‡æ¡£"æŒ‰é’®ï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ†è¯å¹¶æ„å»ºå‘é‡ç´¢å¼•
3. **æ™ºèƒ½é—®ç­”**: åœ¨ä¸»ç•Œé¢è¾“å…¥é—®é¢˜ï¼Œè·å–åŸºäºæ–‡æ¡£å†…å®¹çš„å›ç­”
4. **æ–‡æ¡£ç­›é€‰**: é€‰æ‹©ç‰¹å®šæ–‡æ¡£è¿›è¡Œç²¾ç¡®æŸ¥è¯¢
5. **çŸ¥è¯†åº“ç®¡ç†**: åœ¨ç®¡ç†ç•Œé¢æŸ¥çœ‹å’Œç»´æŠ¤çŸ¥è¯†åº“

## ğŸ” RAG æŠ€æœ¯è¯¦è§£

### åˆ†è¯ç­–ç•¥ (Text Splitting)

é¡¹ç›®é‡‡ç”¨ LangChain çš„ `RecursiveCharacterTextSplitter`ï¼Œå®ç°æ™ºèƒ½åˆ†è¯ï¼š

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # æ¯ä¸ªæ–‡æœ¬å—å¤§å°
    chunk_overlap=50,      # å—é—´é‡å å­—ç¬¦æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", " ", ""]  # åˆ†å‰²ä¼˜å…ˆçº§
)
```

**åˆ†è¯ç‰¹ç‚¹**:

- **é€’å½’åˆ†å‰²**: æŒ‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§é€’å½’åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- **æ™ºèƒ½é‡å **: 50å­—ç¬¦é‡å ç¡®ä¿ä¸Šä¸‹æ–‡è¿ç»­æ€§
- **ä¸­æ–‡ä¼˜åŒ–**: é’ˆå¯¹ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–åˆ†å‰²é€»è¾‘
- **å…ƒæ•°æ®ä¿æŒ**: æ¯ä¸ªæ–‡æœ¬å—ä¿ç•™å®Œæ•´çš„æºæ–‡æ¡£ä¿¡æ¯

### æ£€ç´¢æœºåˆ¶ (Retrieval)

#### å‘é‡åŒ–å­˜å‚¨

- **åµŒå…¥æ¨¡å‹**: Silicon Flow BGE-M3 æ¨¡å‹
- **å‘é‡æ•°æ®åº“**: ChromaDB æŒä¹…åŒ–å­˜å‚¨
- **ç›¸ä¼¼åº¦è®¡ç®—**: ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…

#### æ£€ç´¢æµç¨‹

```python
# 1. é—®é¢˜å‘é‡åŒ–
query_embedding = embedding_function(question)

# 2. ç›¸ä¼¼åº¦æ£€ç´¢
results = collection.query(
    query_texts=[question],
    n_results=5,           # è¿”å›æœ€ç›¸å…³çš„5ä¸ªæ–‡æ¡£å—
    include=["documents", "metadatas"]
)

# 3. æ–‡æ¡£è¿‡æ»¤ (å¯é€‰)
if document_filters:
    # ç²¾ç¡®æ–‡æ¡£åŒ¹é…æˆ–æ¨¡ç³ŠåŒ¹é…
    filtered_results = apply_document_filters(results, filters)
```

#### æ£€ç´¢æ¨¡å¼

- **å…¨å±€æ£€ç´¢**: åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æœç´¢ç›¸å…³å†…å®¹
- **æ–‡æ¡£è¿‡æ»¤**: é™å®šç‰¹å®šæ–‡æ¡£èŒƒå›´å†…æ£€ç´¢
- **ä¸¥æ ¼æ¨¡å¼**: ä»…åœ¨é€‰å®šæ–‡æ¡£ä¸­ç²¾ç¡®åŒ¹é…
- **ç›¸ä¼¼åŒ¹é…**: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ¨¡ç³ŠåŒ¹é…

### å¤åˆé—®é¢˜å¤„ç†

ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«åŒ…å«å¤šä¸ªé—®å·æˆ–åˆ†å·çš„å¤åˆé—®é¢˜ï¼š

```python
def split_compound_question(question):
    # æ£€æµ‹å¤šä¸ªé—®å·æˆ–åˆ†å·
    delimiters = ['ï¼Ÿ', '?', 'ï¼›', ';']
    if count_delimiters(question) > 1:
        # æ‹†åˆ†ä¸ºå­é—®é¢˜
        sub_questions = split_by_delimiters(question)
        # åˆ†åˆ«æ£€ç´¢ï¼Œåˆå¹¶ç»“æœ
        return sub_questions
    return [question]
```

### ä¸Šä¸‹æ–‡æ„å»º

æ™ºèƒ½ä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†ï¼Œç¡®ä¿ LLM ç”Ÿæˆè´¨é‡ï¼š

```python
MAX_CONTEXT_LENGTH = 8000  # é…ç½®çš„ä¸Šä¸‹æ–‡é™åˆ¶

# æŒ‰æ–‡æ¡£é•¿åº¦æ’åºï¼Œä¼˜å…ˆä¿ç•™çŸ­æ–‡æ¡£
contexts.sort(key=lambda x: len(x))

# åŠ¨æ€æ·»åŠ ä¸Šä¸‹æ–‡ç›´åˆ°è¾¾åˆ°é•¿åº¦é™åˆ¶
for context in contexts:
    if total_length + len(context) <= MAX_CONTEXT_LENGTH:
        final_contexts.append(context)
        total_length += len(context)
```

## ğŸ¯ ä¼˜åŒ–å»ºè®®

### 1. åˆ†è¯ä¼˜åŒ–

#### å½“å‰å®ç°

```python
chunk_size=500, chunk_overlap=50
```

#### ä¼˜åŒ–æ–¹æ¡ˆ

```python
# æ ¹æ®æ–‡æ¡£ç±»å‹åŠ¨æ€è°ƒæ•´
def get_optimal_chunk_config(file_type):
    configs = {
        'pdf': {'chunk_size': 800, 'chunk_overlap': 80},    # å­¦æœ¯æ–‡æ¡£
        'md': {'chunk_size': 600, 'chunk_overlap': 60},     # æŠ€æœ¯æ–‡æ¡£  
        'txt': {'chunk_size': 400, 'chunk_overlap': 40},    # ä¸€èˆ¬æ–‡æœ¬
        'docx': {'chunk_size': 700, 'chunk_overlap': 70}    # å•†åŠ¡æ–‡æ¡£
    }
    return configs.get(file_type, {'chunk_size': 500, 'chunk_overlap': 50})
```

#### è¯­ä¹‰åˆ†å‰²å¢å¼º

```python
# é›†æˆå¥å­çº§åˆ«çš„è¯­ä¹‰åˆ†å‰²
from sentence_transformers import SentenceTransformer

def semantic_chunking(text, max_chunk_size=500):
    sentences = split_into_sentences(text)
    chunks = []
    current_chunk = []
    current_size = 0
  
    for sentence in sentences:
        if current_size + len(sentence) > max_chunk_size and current_chunk:
            chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_size = len(sentence)
        else:
            current_chunk.append(sentence)
            current_size += len(sentence)
  
    if current_chunk:
        chunks.append(' '.join(current_chunk))
  
    return chunks
```

### 2. æ£€ç´¢ç²¾åº¦ä¼˜åŒ–

#### æ··åˆæ£€ç´¢ç­–ç•¥

```python
def hybrid_retrieval(question, collection, alpha=0.7):
    # å‘é‡æ£€ç´¢
    vector_results = collection.query(query_texts=[question], n_results=10)
  
    # BM25 å…³é”®è¯æ£€ç´¢
    bm25_results = bm25_search(question, collection)
  
    # ç»“æœèåˆ (RRF - Reciprocal Rank Fusion)
    final_results = reciprocal_rank_fusion(
        vector_results, bm25_results, alpha=alpha
    )
    return final_results
```

#### é‡æ’åºæœºåˆ¶

```python
def rerank_results(question, results, rerank_model):
    # ä½¿ç”¨ä¸“é—¨çš„é‡æ’åºæ¨¡å‹
    pairs = [(question, doc) for doc in results['documents'][0]]
    scores = rerank_model.compute_score(pairs)
  
    # æŒ‰é‡æ’åºåˆ†æ•°é‡æ–°æ’åˆ—
    sorted_indices = sorted(range(len(scores)), 
                          key=lambda i: scores[i], reverse=True)
  
    return reorder_results(results, sorted_indices)
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### å‘é‡ç´¢å¼•ä¼˜åŒ–

```python
# ChromaDB é…ç½®ä¼˜åŒ–
collection = client.create_collection(
    name="documents",
    embedding_function=embedding_function,
    metadata={
        "hnsw:space": "cosine",           # ä½™å¼¦ç›¸ä¼¼åº¦
        "hnsw:construction_ef": 200,      # æ„å»ºæ—¶é‚»å±…æ•°
        "hnsw:M": 16,                     # è¿æ¥æ•°
        "hnsw:search_ef": 100             # æœç´¢æ—¶é‚»å±…æ•°
    }
)
```

#### ç¼“å­˜æœºåˆ¶

```python
# æŸ¥è¯¢ç»“æœç¼“å­˜
@lru_cache(maxsize=1000)
def cached_query(question_hash, doc_filters_hash):
    return collection.query(...)

# åµŒå…¥å‘é‡ç¼“å­˜
class CachedEmbeddingFunction:
    def __init__(self, base_function):
        self.base_function = base_function
        self.cache = {}
  
    def __call__(self, texts):
        cached_results = []
        new_texts = []
      
        for text in texts:
            text_hash = hash(text)
            if text_hash in self.cache:
                cached_results.append(self.cache[text_hash])
            else:
                new_texts.append(text)
      
        if new_texts:
            new_embeddings = self.base_function(new_texts)
            for text, embedding in zip(new_texts, new_embeddings):
                self.cache[hash(text)] = embedding
                cached_results.append(embedding)
      
        return cached_results
```

### 4. å†…å®¹è´¨é‡ä¼˜åŒ–

#### æ–‡æ¡£é¢„å¤„ç†å¢å¼º

```python
def enhanced_document_preprocessing(file_path, file_type):
    if file_type == 'pdf':
        # OCR å¤„ç†æ‰«æç‰ˆPDF
        if is_scanned_pdf(file_path):
            text = ocr_pdf(file_path)
        else:
            text = extract_pdf_text(file_path)
      
        # æ¸…ç†PDFç‰¹æœ‰å™ªå£°
        text = clean_pdf_artifacts(text)
      
    elif file_type == 'docx':
        # ä¿ç•™æ ¼å¼ä¿¡æ¯
        text, formatting = extract_docx_with_formatting(file_path)
      
    # é€šç”¨æ–‡æœ¬æ¸…ç†
    text = clean_common_artifacts(text)
  
    return text
```

#### æ™ºèƒ½å…ƒæ•°æ®æå–

```python
def extract_smart_metadata(document_text, file_path):
    metadata = {
        'source': file_path,
        'filename': os.path.basename(file_path),
        'file_type': get_file_extension(file_path),
        'timestamp': int(time.time()),
      
        # å†…å®¹åˆ†æ
        'language': detect_language(document_text),
        'topics': extract_topics(document_text),
        'entities': extract_entities(document_text),
        'word_count': len(document_text.split()),
        'reading_time': estimate_reading_time(document_text)
    }
    return metadata
```

### 5. å¤šæ¨¡æ€æ‰©å±•

#### å›¾åƒå†…å®¹ç†è§£

```python
# ä¸ºåŒ…å«å›¾åƒçš„æ–‡æ¡£æ·»åŠ è§†è§‰ç†è§£
def process_document_with_images(file_path):
    if file_type == 'pdf':
        images = extract_images_from_pdf(file_path)
        for image in images:
            # ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ç†è§£å›¾åƒ
            image_description = vision_model.describe(image)
            # å°†å›¾åƒæè¿°ä½œä¸ºæ–‡æ¡£å†…å®¹çš„ä¸€éƒ¨åˆ†
            document_text += f"\n[å›¾åƒæè¿°: {image_description}]"
  
    return document_text
```

### 6. ç”¨æˆ·ä½“éªŒä¼˜åŒ–

#### æ™ºèƒ½é—®é¢˜å»ºè®®

```python
def suggest_questions(documents):
    # åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆæ¨èé—®é¢˜
    questions = []
    for doc in documents:
        # æå–å…³é”®ä¸»é¢˜
        topics = extract_key_topics(doc.content)
        # ç”Ÿæˆç›¸å…³é—®é¢˜
        for topic in topics:
            suggested = generate_questions_for_topic(topic)
            questions.extend(suggested)
  
    return deduplicate_questions(questions)
```

#### å¯¹è¯å†å²ç®¡ç†

```python
class ConversationManager:
    def __init__(self):
        self.history = []
        self.context_memory = {}
  
    def add_interaction(self, question, answer, context_docs):
        self.history.append({
            'question': question,
            'answer': answer,
            'context_docs': context_docs,
            'timestamp': time.time()
        })
  
    def get_relevant_history(self, current_question):
        # è¿”å›ä¸å½“å‰é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯
        return semantic_search_history(current_question, self.history)
```

## ğŸ”§ é«˜çº§é…ç½®

### æ¨¡å‹é…ç½®ä¼˜åŒ–

ç¼–è¾‘ `config.py` æ–‡ä»¶è¿›è¡Œé«˜çº§é…ç½®ï¼š

```python
# æ€§èƒ½è°ƒä¼˜
APP_SETTINGS = {
    "max_context_length": 12000,    # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
    "max_tokens": 6000,             # å¢åŠ ç”Ÿæˆé•¿åº¦
    "temperature": 0.1,             # é™ä½éšæœºæ€§
    "request_timeout": 60,          # å¢åŠ è¶…æ—¶æ—¶é—´
    "max_retries": 3,               # å¢åŠ é‡è¯•æ¬¡æ•°
  
    # æ£€ç´¢é…ç½®
    "retrieval_top_k": 8,           # æ£€ç´¢æ–‡æ¡£æ•°é‡
    "rerank_top_k": 5,              # é‡æ’åºåä¿ç•™æ•°é‡
    "similarity_threshold": 0.7,     # ç›¸ä¼¼åº¦é˜ˆå€¼
}
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **çŸ¥è¯†åº“æŸå**

   ```bash
   # åœ¨åº”ç”¨ä¸­ç‚¹å‡»"å¼ºåˆ¶é‡å»ºçŸ¥è¯†åº“"
   # æˆ–æ‰‹åŠ¨åˆ é™¤æ•°æ®åº“ç›®å½•
   rm -rf ./chroma_db
   ```
2. **å†…å­˜ä¸è¶³**

   - å‡å°‘ `chunk_size` å‚æ•°
   - é™åˆ¶åŒæ—¶å¤„ç†çš„æ–‡æ¡£æ•°é‡
   - å¢åŠ ç³»ç»Ÿå†…å­˜
3. **API è°ƒç”¨å¤±è´¥**

   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - éªŒè¯ API å¯†é’¥æœ‰æ•ˆæ€§
   - æŸ¥çœ‹ API ä½™é¢å’Œè°ƒç”¨é™åˆ¶

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯: `git checkout -b feature/new-feature`
3. æäº¤æ›´æ”¹: `git commit -m 'Add new feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/new-feature`
5. æäº¤ Pull Request

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ‘¨â€ğŸ’» ä½œè€…

**haojiefang** - é¡¹ç›®åˆ›å»ºè€…å’Œä¸»è¦ç»´æŠ¤è€…

---

## ğŸ™ è‡´è°¢

- [LangChain](https://github.com/langchain-ai/langchain) - å¼ºå¤§çš„ LLM åº”ç”¨æ¡†æ¶
- [ChromaDB](https://github.com/chroma-core/chroma) - é«˜æ€§èƒ½å‘é‡æ•°æ®åº“
- [Streamlit](https://github.com/streamlit/streamlit) - ä¼˜é›…çš„ Web åº”ç”¨æ¡†æ¶
- [Silicon Flow](https://siliconflow.cn/) - é«˜è´¨é‡åµŒå…¥æ¨¡å‹æœåŠ¡

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– Pull Requestï¼
